{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dabbbc",
   "metadata": {},
   "source": [
    "0. Imports & basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96554203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\System-Data-Local\\Compilers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ff5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a4220",
   "metadata": {},
   "source": [
    "1. Config (easy to tweak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = r\"F:\\Workspace\\Working\\college-minor-project\\datasets\\Disease Grading\\Original Images\\Training Set\"\n",
    "TEST_DIR  = r\"F:\\Workspace\\Working\\college-minor-project\\datasets\\Disease Grading\\Original Images\\Testing Set\"\n",
    "TRAIN_CSV = r\"F:\\Workspace\\Working\\college-minor-project\\datasets\\Disease Grading\\Groundtruths\\a. IDRiD_Disease Grading_Training Labels.csv\"\n",
    "TEST_CSV  = r\"F:\\Workspace\\Working\\college-minor-project\\datasets\\Disease Grading\\Groundtruths\\b. IDRiD_Disease Grading_Testing Labels.csv\"\n",
    "\n",
    "\n",
    "# ---- Paths (preprocessed) ----\n",
    "PREP_ROOT       = Path(\"preprocessed_idrid\")\n",
    "PREP_TRAIN_DIR  = PREP_ROOT / \"train\"\n",
    "PREP_TEST_DIR   = PREP_ROOT / \"test\"\n",
    "PREP_TRAIN_CSV  = PREP_ROOT / \"train_preprocessed.csv\"\n",
    "PREP_TEST_CSV   = PREP_ROOT / \"test_preprocessed.csv\"\n",
    "\n",
    "# ---- Outputs ----\n",
    "RUN_DIR   = Path(\"runs/idrid_simple\")\n",
    "CKPT_PATH = Path(\"checkpoints/best_idrid_simple.pt\")\n",
    "\n",
    "# Model / training\n",
    "MODEL_NAME = \"mobilenetv3_large_100\"   \n",
    "IMG_SIZE   = 320                      # smaller than 512 → faster\n",
    "#NUM_CLASSES = 5\n",
    "\n",
    "NUM_DR_CLASSES = 5          # Retinopathy grade: 0..4\n",
    "NUM_DME_CLASSES = 3         # Risk of macular edema: 0..2\n",
    "\n",
    "LOSS_WEIGHT_DME = 1.0       # you can tune this (e.g. 0.5) if one task dominates\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EPOCHS = 20\n",
    "VAL_SPLIT = 0.2\n",
    "LR_WARMUP = 4e-4               # new\n",
    "LR_FINETUNE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 7\n",
    "\n",
    "IMBALANCE_STRATEGY = \"sampler\"         # \"sampler\", \"class_weights\", \"none\"\n",
    "WARMUP_EPOCHS = 2 \n",
    "\n",
    "SEED = 24\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551491d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NPZ = PREP_ROOT / \"train_arrays.npz\"\n",
    "TEST_NPZ  = PREP_ROOT / \"test_arrays.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6c5a0",
   "metadata": {},
   "source": [
    "1. Basic helpers (minimal functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c451ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_column(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    lowered = {c.lower().strip(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        key = c.lower().strip()\n",
    "        if key in lowered:\n",
    "            return lowered[key]\n",
    "    raise ValueError(f\"CSV missing expected columns. Tried {candidates}. Found: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae66823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundus_bbox_square(img: np.ndarray, pad_ratio: float = 0.01):\n",
    "    \"\"\"\n",
    "    Robust crop that tries not to cut off the fundus:\n",
    "    1) mask non-dark pixels\n",
    "    2) bounding rect of mask\n",
    "    3) pad to square with black borders\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        thr = max(5, int(np.percentile(gray, 5)))\n",
    "        mask = (gray > thr).astype(np.uint8) * 255\n",
    "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not cnts:\n",
    "            return img\n",
    "        x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))\n",
    "        H, W = img.shape[:2]\n",
    "        padx = int(w * pad_ratio)\n",
    "        pady = int(h * pad_ratio)\n",
    "        x1 = max(0, x - padx)\n",
    "        y1 = max(0, y - pady)\n",
    "        x2 = min(W, x + w + padx)\n",
    "        y2 = min(H, y + h + pady)\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        hh, ww = crop.shape[:2]\n",
    "        if hh == ww:\n",
    "            return crop\n",
    "        side = max(hh, ww)\n",
    "        top = (side - hh) // 2\n",
    "        bottom = side - hh - top\n",
    "        left = (side - ww) // 2\n",
    "        right = side - ww - left\n",
    "        crop_sq = cv2.copyMakeBorder(\n",
    "            crop, top, bottom, left, right,\n",
    "            borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "        )\n",
    "        return crop_sq\n",
    "    except Exception:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe8b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retina_enhance(rgb: np.ndarray):\n",
    "    \"\"\"Simple Ben Graham style shade correction + mild unsharp mask.\"\"\"\n",
    "    blur = cv2.GaussianBlur(rgb, (0, 0), sigmaX=rgb.shape[1] * 0.05)\n",
    "    out = cv2.addWeighted(rgb, 4.0, blur, -4.0, 128)\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28d603",
   "metadata": {},
   "source": [
    "2. Init seed & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4de516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. This script is written for GPU training only.\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"[Info] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0c886",
   "metadata": {},
   "source": [
    "3. Read original CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe88413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(TRAIN_CSV)\n",
    "df_test_raw  = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3988eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_img_col  = find_column(df_train_raw, img_col_candidates)\n",
    "# train_label_col = find_column(df_train_raw, label_col_candidates)\n",
    "# test_img_col   = find_column(df_test_raw, img_col_candidates)\n",
    "# test_label_col  = find_column(df_test_raw, label_col_candidates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a89e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_raw[[\"Image name\", \"Retinopathy grade\", \"Risk of macular edema\"]].copy()\n",
    "df_train.columns = [\"image\", \"label_dr\", \"label_dme\"]\n",
    "\n",
    "df_test = df_test_raw[[\"Image name\", \"Retinopathy grade\", \"Risk of macular edema \"]].copy()\n",
    "df_test.columns = [\"image\", \"label_dr\", \"label_dme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c581d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_ext(x):\n",
    "    x = str(x)\n",
    "    if any(x.lower().endswith(ext) for ext in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]):\n",
    "        return x\n",
    "    return x + \".jpg\"\n",
    "\n",
    "df_train[\"image\"]    = df_train[\"image\"].apply(ensure_ext)\n",
    "df_test[\"image\"]     = df_test[\"image\"].apply(ensure_ext)\n",
    "\n",
    "df_train[\"label_dr\"]  = df_train[\"label_dr\"].astype(int)\n",
    "df_train[\"label_dme\"] = df_train[\"label_dme\"].astype(int)\n",
    "\n",
    "df_test[\"label_dr\"]   = df_test[\"label_dr\"].astype(int)\n",
    "df_test[\"label_dme\"]  = df_test[\"label_dme\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703ba3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Original -> Train rows: 413, Test rows: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Info] Original -> Train rows: {len(df_train)}, Test rows: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c10717",
   "metadata": {},
   "source": [
    "4. Preprocess & save images (only once)\n",
    "- crop, enhance, resize\n",
    "- save to PREP_TRAIN_DIR / PREP_TEST_DIR\n",
    "- write new CSVs with full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b072d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.iterrows of              image  label_dr  label_dme\n",
       "0    IDRiD_001.jpg         3          2\n",
       "1    IDRiD_002.jpg         3          2\n",
       "2    IDRiD_003.jpg         2          2\n",
       "3    IDRiD_004.jpg         3          2\n",
       "4    IDRiD_005.jpg         4          0\n",
       "..             ...       ...        ...\n",
       "408  IDRiD_409.jpg         2          1\n",
       "409  IDRiD_410.jpg         2          0\n",
       "410  IDRiD_411.jpg         2          0\n",
       "411  IDRiD_412.jpg         2          0\n",
       "412  IDRiD_413.jpg         2          0\n",
       "\n",
       "[413 rows x 3 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iterrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7de26f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Preprocessing images (this is the slow part, done once)...\n",
      "  [Train] 50/413\n",
      "  [Train] 100/413\n",
      "  [Train] 150/413\n",
      "  [Train] 200/413\n",
      "  [Train] 250/413\n",
      "  [Train] 300/413\n",
      "  [Train] 350/413\n",
      "  [Train] 400/413\n",
      "[Info] Saved preprocessed train CSV: preprocessed_idrid\\train_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "PREP_TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREP_TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if PREP_TRAIN_CSV.exists() and PREP_TEST_CSV.exists():\n",
    "    print(f\"[Info] Preprocessed CSVs already exist: {PREP_TRAIN_CSV}, {PREP_TEST_CSV}\")\n",
    "    df_train_prep = pd.read_csv(PREP_TRAIN_CSV)\n",
    "    df_test_prep  = pd.read_csv(PREP_TEST_CSV)\n",
    "else:\n",
    "    print(\"[Info] Preprocessing images (this is the slow part, done once)...\")\n",
    "\n",
    "    # ---- preprocess TRAIN ----\n",
    "    paths_prep      = []\n",
    "    labels_dr_prep  = []\n",
    "    labels_dme_prep = []\n",
    "\n",
    "    for i, row in df_train.iterrows():\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  [Train] {i+1}/{len(df_train)}\")\n",
    "\n",
    "        raw_path = Path(TRAIN_DIR) / row[\"image\"]\n",
    "        if not raw_path.exists():\n",
    "            print(f\"  [WARN] Missing train image: {raw_path}\")\n",
    "            continue\n",
    "\n",
    "        bgr = cv2.imread(str(raw_path), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            print(f\"  [WARN] Cannot read train image: {raw_path}\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # crop + enhance + resize\n",
    "        rgb = fundus_bbox_square(rgb, pad_ratio=0.01)\n",
    "        rgb = retina_enhance(rgb)\n",
    "        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # save\n",
    "        out_name = f\"train_{i:04d}.jpg\"\n",
    "        out_path = PREP_TRAIN_DIR / out_name\n",
    "        cv2.imwrite(str(out_path), cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        paths_prep.append(str(out_path.resolve()))\n",
    "        labels_dr_prep.append(int(row[\"label_dr\"]))\n",
    "        labels_dme_prep.append(int(row[\"label_dme\"]))\n",
    "\n",
    "    df_train_prep = pd.DataFrame({\n",
    "        \"path\": paths_prep,\n",
    "        \"label_dr\": labels_dr_prep,\n",
    "        \"label_dme\": labels_dme_prep,\n",
    "    })\n",
    "    df_train_prep.to_csv(PREP_TRAIN_CSV, index=False)\n",
    "    print(f\"[Info] Saved preprocessed train CSV: {PREP_TRAIN_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4ba44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Test] 50/103\n",
      "  [Test] 100/103\n",
      "[Info] Saved preprocessed test CSV: preprocessed_idrid\\test_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- preprocess TEST ----\n",
    "paths_prep      = []\n",
    "labels_dr_prep  = []\n",
    "labels_dme_prep = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  [Test] {i+1}/{len(df_test)}\")\n",
    "\n",
    "    raw_path = Path(TEST_DIR) / row[\"image\"]\n",
    "    if not raw_path.exists():\n",
    "        print(f\"  [WARN] Missing test image: {raw_path}\")\n",
    "        continue\n",
    "\n",
    "    bgr = cv2.imread(str(raw_path), cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        print(f\"  [WARN] Cannot read test image: {raw_path}\")\n",
    "        continue\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    rgb = fundus_bbox_square(rgb, pad_ratio=0.01)\n",
    "    rgb = retina_enhance(rgb)\n",
    "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    out_name = f\"test_{i:04d}.jpg\"\n",
    "    out_path = PREP_TEST_DIR / out_name\n",
    "    cv2.imwrite(str(out_path), cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    paths_prep.append(str(out_path.resolve()))\n",
    "    labels_dr_prep.append(int(row[\"label_dr\"]))\n",
    "    labels_dme_prep.append(int(row[\"label_dme\"]))\n",
    "\n",
    "df_test_prep = pd.DataFrame({\n",
    "    \"path\": paths_prep,\n",
    "    \"label_dr\": labels_dr_prep,\n",
    "    \"label_dme\": labels_dme_prep,\n",
    "})\n",
    "df_test_prep.to_csv(PREP_TEST_CSV, index=False)\n",
    "print(f\"[Info] Saved preprocessed test CSV: {PREP_TEST_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffad46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Running targeted augmentation for DR grade-1 (4 extra samples per original).\n",
      "[Info] Found 40 DR grade-1 images for augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\System-Data-Local\\Compilers\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DR1-Aug] 10/40 originals done\n",
      "  [DR1-Aug] 20/40 originals done\n",
      "  [DR1-Aug] 30/40 originals done\n",
      "  [DR1-Aug] 40/40 originals done\n",
      "[Info] Added 160 augmented DR=1 samples. New train size: 986\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4b. Targeted offline augmentation for DR grade-1 ONLY\n",
    "#     (run ONCE; then comment out this block or set flag False)\n",
    "# ============================================================\n",
    "DO_DR1_AUG = True          # set False after you’ve run once\n",
    "N_AUG_PER_DR1 = 4          # 3–5 is reasonable\n",
    "\n",
    "if DO_DR1_AUG:\n",
    "    import albumentations as A\n",
    "\n",
    "    print(f\"[Info] Running targeted augmentation for DR grade-1 \"\n",
    "          f\"({N_AUG_PER_DR1} extra samples per original).\")\n",
    "\n",
    "    # mild, realistic augmentations\n",
    "    dr1_aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(0.1, 0.1, p=0.7),\n",
    "        A.ShiftScaleRotate(shift_limit=0.02,\n",
    "                           scale_limit=0.05,\n",
    "                           rotate_limit=10,\n",
    "                           border_mode=cv2.BORDER_REFLECT101,\n",
    "                           p=0.7),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.2)\n",
    "    ])\n",
    "\n",
    "    # filter DR grade-1 rows\n",
    "    df_dr1 = df_train_prep[df_train_prep[\"label_dr\"] == 1].reset_index(drop=True)\n",
    "    print(f\"[Info] Found {len(df_dr1)} DR grade-1 images for augmentation.\")\n",
    "\n",
    "    extra_rows = []\n",
    "\n",
    "    for i, row in df_dr1.iterrows():\n",
    "        img_path = Path(row[\"path\"])\n",
    "        bgr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            print(f\"[WARN] Cannot read DR1 image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        for k in range(N_AUG_PER_DR1):\n",
    "            augmented = dr1_aug(image=rgb)[\"image\"]\n",
    "\n",
    "            out_name = f\"{img_path.stem}_dr1aug{k}.jpg\"\n",
    "            out_path = PREP_TRAIN_DIR / out_name\n",
    "            cv2.imwrite(str(out_path), cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            extra_rows.append({\n",
    "                \"path\": str(out_path.resolve()),\n",
    "                \"label_dr\": int(row[\"label_dr\"]),      # always 1\n",
    "                \"label_dme\": int(row[\"label_dme\"])     # keep same DME label\n",
    "            })\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  [DR1-Aug] {i+1}/{len(df_dr1)} originals done\")\n",
    "\n",
    "    if extra_rows:\n",
    "        df_extra = pd.DataFrame(extra_rows)\n",
    "        df_train_prep = pd.concat([df_train_prep, df_extra], ignore_index=True)\n",
    "        df_train_prep.to_csv(PREP_TRAIN_CSV, index=False)\n",
    "        print(f\"[Info] Added {len(df_extra)} augmented DR=1 samples. \"\n",
    "              f\"New train size: {len(df_train_prep)}\")\n",
    "    else:\n",
    "        print(\"[Info] No extra DR1 samples created (check warnings above).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959d9cc",
   "metadata": {},
   "source": [
    "5. OPTIONAL: Albumentations offline augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b029ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALBUMENTATIONS_OFFLINE = True  # <-- set True if you want to run this block ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55b6f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Running offline Albumentations to create extra augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\System-Data-Local\\Compilers\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Aug] 50/413\n",
      "  [Aug] 100/413\n",
      "  [Aug] 150/413\n",
      "  [Aug] 200/413\n",
      "  [Aug] 250/413\n",
      "  [Aug] 300/413\n",
      "  [Aug] 350/413\n",
      "  [Aug] 400/413\n",
      "[Info] Albumentations added 413 samples. New train size: 826\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "\n",
    "print(\"[Info] Running offline Albumentations to create extra augmented images...\")\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(0.1, 0.1, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=10, border_mode=cv2.BORDER_REFLECT101, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3,5), p=0.2)\n",
    "])\n",
    "\n",
    "extra_paths      = []\n",
    "extra_labels_dr  = []\n",
    "extra_labels_dme = []\n",
    "\n",
    "for i, row in df_train_prep.iterrows():\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  [Aug] {i+1}/{len(df_train_prep)}\")\n",
    "    img_path = Path(row[\"path\"])\n",
    "    bgr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        continue\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    augmented = aug(image=rgb)[\"image\"]\n",
    "    out_name = img_path.stem + \"_aug.jpg\"\n",
    "    out_path = PREP_TRAIN_DIR / out_name\n",
    "    cv2.imwrite(str(out_path), cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "    extra_paths.append(str(out_path.resolve()))\n",
    "    extra_labels_dr.append(int(row[\"label_dr\"]))\n",
    "    extra_labels_dme.append(int(row[\"label_dme\"]))\n",
    "\n",
    "if extra_paths:\n",
    "    df_extra = pd.DataFrame({\n",
    "        \"path\": extra_paths,\n",
    "        \"label_dr\": extra_labels_dr,\n",
    "        \"label_dme\": extra_labels_dme,\n",
    "    })\n",
    "    df_train_prep = pd.concat([df_train_prep, df_extra], ignore_index=True)\n",
    "    df_train_prep.to_csv(PREP_TRAIN_CSV, index=False)\n",
    "    print(f\"[Info] Albumentations added {len(extra_paths)} samples. New train size: {len(df_train_prep)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648c219",
   "metadata": {},
   "source": [
    "6. Load preprocessed images into memory & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e60025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] NPZ cache not found. Loading JPGs and building arrays (one-time)...\n",
      "  [Load-Train] 100/986\n",
      "  [Load-Train] 200/986\n",
      "  [Load-Train] 300/986\n",
      "  [Load-Train] 400/986\n",
      "  [Load-Train] 500/986\n",
      "  [Load-Train] 600/986\n",
      "  [Load-Train] 700/986\n",
      "  [Load-Train] 800/986\n",
      "  [Load-Train] 900/986\n",
      "  [Load-Test] 100/103\n",
      "[Info] Saved NPZ cache: preprocessed_idrid\\train_arrays.npz, preprocessed_idrid\\test_arrays.npz\n",
      "[Info] X_train shape: (986, 320, 320, 3), y_train_dr shape: (986,), y_train_dme shape: (986,)\n",
      "[Info] X_test shape:  (103, 320, 320, 3),  y_test_dr shape:  (103,),  y_test_dme shape:  (103,)\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_NPZ.exists() and TEST_NPZ.exists():\n",
    "    print(f\"[Info] NPZ cache found: {TRAIN_NPZ}, {TEST_NPZ}\")\n",
    "    train_data = np.load(TRAIN_NPZ)\n",
    "    test_data  = np.load(TEST_NPZ)\n",
    "\n",
    "    X_train     = train_data[\"X\"]\n",
    "    y_train_dr  = train_data[\"y_dr\"]\n",
    "    y_train_dme = train_data[\"y_dme\"]\n",
    "\n",
    "    X_test      = test_data[\"X\"]\n",
    "    y_test_dr   = test_data[\"y_dr\"]\n",
    "    y_test_dme  = test_data[\"y_dme\"]\n",
    "\n",
    "else:\n",
    "    print(\"[Info] NPZ cache not found. Loading JPGs and building arrays (one-time)...\")\n",
    "\n",
    "    # ---- build train arrays ----\n",
    "    X_list      = []\n",
    "    y_dr_list   = []\n",
    "    y_dme_list  = []\n",
    "    for i, row in df_train_prep.iterrows():\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  [Load-Train] {i+1}/{len(df_train_prep)}\")\n",
    "        path = row[\"path\"]\n",
    "        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            print(f\"[WARN] Cannot read preprocessed train image: {path}\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        X_list.append(rgb)\n",
    "        y_dr_list.append(int(row[\"label_dr\"]))\n",
    "        y_dme_list.append(int(row[\"label_dme\"]))\n",
    "\n",
    "    X_train     = np.stack(X_list, axis=0)\n",
    "    y_train_dr  = np.array(y_dr_list, dtype=np.int64)\n",
    "    y_train_dme = np.array(y_dme_list, dtype=np.int64)\n",
    "\n",
    "    # ---- build test arrays ----\n",
    "    X_list      = []\n",
    "    y_dr_list   = []\n",
    "    y_dme_list  = []\n",
    "    for i, row in df_test_prep.iterrows():\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  [Load-Test] {i+1}/{len(df_test_prep)}\")\n",
    "        path = row[\"path\"]\n",
    "        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            print(f\"[WARN] Cannot read preprocessed test image: {path}\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        X_list.append(rgb)\n",
    "        y_dr_list.append(int(row[\"label_dr\"]))\n",
    "        y_dme_list.append(int(row[\"label_dme\"]))\n",
    "\n",
    "    X_test      = np.stack(X_list, axis=0)\n",
    "    y_test_dr   = np.array(y_dr_list, dtype=np.int64)\n",
    "    y_test_dme  = np.array(y_dme_list, dtype=np.int64)\n",
    "\n",
    "    # save NPZ so future runs are INSTANT\n",
    "    np.savez(TRAIN_NPZ, X=X_train, y_dr=y_train_dr, y_dme=y_train_dme)\n",
    "    np.savez(TEST_NPZ,  X=X_test,  y_dr=y_test_dr,  y_dme=y_test_dme)\n",
    "    print(f\"[Info] Saved NPZ cache: {TRAIN_NPZ}, {TEST_NPZ}\")\n",
    "\n",
    "print(f\"[Info] X_train shape: {X_train.shape}, y_train_dr shape: {y_train_dr.shape}, y_train_dme shape: {y_train_dme.shape}\")\n",
    "print(f\"[Info] X_test shape:  {X_test.shape},  y_test_dr shape:  {y_test_dr.shape},  y_test_dme shape:  {y_test_dme.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71464df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loading preprocessed train images into RAM...\n",
      "  [Load] 100/826\n",
      "  [Load] 200/826\n",
      "  [Load] 300/826\n",
      "  [Load] 400/826\n",
      "  [Load] 500/826\n",
      "  [Load] 600/826\n",
      "  [Load] 700/826\n",
      "  [Load] 800/826\n",
      "[Info] Loading preprocessed test images into RAM...\n",
      "  [Load] 100/103\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_df(df, img_size):\n",
    "    X_list      = []\n",
    "    y_dr_list   = []\n",
    "    y_dme_list  = []\n",
    "    for i, row in df.iterrows():\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  [Load] {i+1}/{len(df)}\")\n",
    "        path = row[\"path\"]\n",
    "        bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            print(f\"[WARN] Cannot read preprocessed image: {path}\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb, (img_size, img_size), interpolation=cv2.INTER_AREA)\n",
    "        X_list.append(rgb)\n",
    "        y_dr_list.append(int(row[\"label_dr\"]))\n",
    "        y_dme_list.append(int(row[\"label_dme\"]))\n",
    "\n",
    "    X    = np.stack(X_list, axis=0)\n",
    "    y_dr = np.array(y_dr_list, dtype=np.int64)\n",
    "    y_dme = np.array(y_dme_list, dtype=np.int64)\n",
    "    return X, y_dr, y_dme\n",
    "\n",
    "print(\"[Info] Loading preprocessed train images into RAM...\")\n",
    "X_train, y_train_dr, y_train_dme = load_images_from_df(df_train_prep, IMG_SIZE)\n",
    "\n",
    "print(\"[Info] Loading preprocessed test images into RAM...\")\n",
    "X_test, y_test_dr, y_test_dme   = load_images_from_df(df_test_prep, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2a1c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] X_train shape: (826, 320, 320, 3), y_train shape: (826,)\n",
      "[Info] X_test shape:  (103, 320, 320, 3),  y_test shape:  (103,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Info] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"[Info] X_test shape:  {X_test.shape},  y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a52a01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test  = X_test.astype(np.float32)  / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81c332a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = IMAGENET_MEAN.reshape(1, 1, 1, 3)\n",
    "std  = IMAGENET_STD.reshape(1, 1, 1, 3)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3be2616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N,H,W,C -> N,C,H,W\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_test  = np.transpose(X_test,  (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5964b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t    = torch.from_numpy(X_train)\n",
    "y_train_dr_t = torch.from_numpy(y_train_dr)\n",
    "y_train_dme_t = torch.from_numpy(y_train_dme)\n",
    "\n",
    "X_test_t     = torch.from_numpy(X_test)\n",
    "y_test_dr_t  = torch.from_numpy(y_test_dr)\n",
    "y_test_dme_t = torch.from_numpy(y_test_dme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fea3441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Tensor shapes -> X_train: torch.Size([986, 3, 320, 320]), X_test: torch.Size([103, 3, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Info] Tensor shapes -> X_train: {X_train_t.shape}, X_test: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c03dd9",
   "metadata": {},
   "source": [
    "7. Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77281484",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=VAL_SPLIT,\n",
    "    random_state=SEED\n",
    ")\n",
    "train_idx, val_idx = next(splitter.split(np.arange(len(y_train_dr)), y_train_dr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4f7d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Split -> Train: 788, Val: 198, Test: 103\n"
     ]
    }
   ],
   "source": [
    "X_tr_t        = X_train_t[train_idx]\n",
    "y_tr_dr_t     = y_train_dr_t[train_idx]\n",
    "y_tr_dme_t    = y_train_dme_t[train_idx]\n",
    "\n",
    "X_va_t        = X_train_t[val_idx]\n",
    "y_va_dr_t     = y_train_dr_t[val_idx]\n",
    "y_va_dme_t    = y_train_dme_t[val_idx]\n",
    "\n",
    "print(f\"[Info] Split -> Train: {len(y_tr_dr_t)}, Val: {len(y_va_dr_t)}, Test: {len(y_test_dr_t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a5180",
   "metadata": {},
   "source": [
    "8. Dataloaders (TensorDataset, no custom Dataset class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2228f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tr_t, y_tr_dr_t, y_tr_dme_t)\n",
    "val_dataset   = TensorDataset(X_va_t, y_va_dr_t, y_va_dme_t)\n",
    "test_dataset  = TensorDataset(X_test_t, y_test_dr_t, y_test_dme_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb52bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = None\n",
    "class_weights_dr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77b1dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMBALANCE_STRATEGY = \"sampler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29551676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Using WeightedRandomSampler (DR labels).\n"
     ]
    }
   ],
   "source": [
    "if IMBALANCE_STRATEGY == \"sampler\":\n",
    "    counts = np.bincount(y_tr_dr_t.numpy(), minlength=NUM_DR_CLASSES)\n",
    "    w_per_sample = 1.0 / (counts[y_tr_dr_t.numpy()] + 1e-6)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=w_per_sample,\n",
    "        num_samples=len(w_per_sample),\n",
    "        replacement=True\n",
    "    )\n",
    "    print(\"[Info] Using WeightedRandomSampler (DR labels).\")\n",
    "elif IMBALANCE_STRATEGY == \"class_weights\":\n",
    "    counts = np.bincount(y_tr_dr_t.numpy(), minlength=NUM_DR_CLASSES).astype(float)\n",
    "    weights = counts.sum() / (counts + 1e-6)\n",
    "    weights = weights / weights.mean()\n",
    "    class_weights_dr = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    print(f\"[Info] Using DR class weights: {class_weights_dr.cpu().numpy()}\")\n",
    "else:\n",
    "    print(\"[Info] No imbalance strategy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8463ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=(sampler is None),\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5485e",
   "metadata": {},
   "source": [
    "9. Model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4477d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES, drop_rate=0.4)\n",
    "# model = model.to(device)\n",
    "# print(f\"[Info] Model '{MODEL_NAME}' created. First param device: {next(model.parameters()).device}\")\n",
    "\n",
    "\n",
    "# # --- Warmup setup: freeze all but classifier head ---\n",
    "# WARMUP_EPOCHS = WARMUP_EPOCHS  # uses the value you set above\n",
    "\n",
    "# # 1) Freeze everything\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# # 2) Unfreeze only classifier / head parameters\n",
    "# for name, p in model.named_parameters():\n",
    "#     if any(k in name.lower() for k in [\"classifier\", \"head\", \"fc\"]):\n",
    "#         p.requires_grad = True\n",
    "\n",
    "# print(\"[Info] Warmup: training only the classifier head for first \"\n",
    "#       f\"{WARMUP_EPOCHS} epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97ba7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Multi-task model created. First param device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskEffNet(nn.Module):\n",
    "    def __init__(self, backbone_name, num_dr, num_dme, drop_rate=0.4):\n",
    "        super().__init__()\n",
    "        # backbone without classifier, global_pool='avg' for pooled features\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='avg',\n",
    "            drop_rate=drop_rate\n",
    "        )\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head_dr  = nn.Linear(feat_dim, num_dr)\n",
    "        self.head_dme = nn.Linear(feat_dim, num_dme)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits_dr  = self.head_dr(feats)\n",
    "        logits_dme = self.head_dme(feats)\n",
    "        return logits_dr, logits_dme\n",
    "\n",
    "model = MultiTaskEffNet(MODEL_NAME, NUM_DR_CLASSES, NUM_DME_CLASSES, drop_rate=0.4).to(device)\n",
    "print(f\"[Info] Multi-task model created. First param device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3116141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dr  = nn.CrossEntropyLoss(weight=class_weights_dr)\n",
    "criterion_dme = nn.CrossEntropyLoss()   # could add weights if DME is imbalanced\n",
    "\n",
    "# Warmup optimizer: head-only\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR_WARMUP,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS - WARMUP_EPOCHS,\n",
    "    eta_min=LR_FINETUNE * 0.1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "116a2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e530b10",
   "metadata": {},
   "source": [
    "10. Training loop (inline, no extra functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "566a1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = -1.0\n",
    "waited = 0\n",
    "\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_PATH.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd8f9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Epoch 01] step 10/66 loss=1.7087\n",
      "  [Epoch 01] step 20/66 loss=1.5197\n",
      "  [Epoch 01] step 30/66 loss=1.1206\n",
      "  [Epoch 01] step 40/66 loss=0.9460\n",
      "  [Epoch 01] step 50/66 loss=0.8860\n",
      "  [Epoch 01] step 60/66 loss=1.8964\n",
      "[Epoch 01] train_loss=1.3697  val_f1_DR=0.6381  val_acc_DR=0.6566  val_f1_DME=0.6632  val_acc_DME=0.8889\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 02] step 10/66 loss=0.4349\n",
      "  [Epoch 02] step 20/66 loss=0.9045\n",
      "  [Epoch 02] step 30/66 loss=0.8419\n",
      "  [Epoch 02] step 40/66 loss=0.4711\n",
      "  [Epoch 02] step 50/66 loss=0.3548\n",
      "  [Epoch 02] step 60/66 loss=0.8235\n",
      "[Epoch 02] train_loss=0.6615  val_f1_DR=0.8268  val_acc_DR=0.8485  val_f1_DME=0.8254  val_acc_DME=0.9545\n",
      "  ↳ Saved checkpoint\n",
      "[Info] Warmup finished. Unfreezing backbone for finetuning (LR=0.0001).\n",
      "  [Epoch 03] step 10/66 loss=0.3088\n",
      "  [Epoch 03] step 20/66 loss=0.4332\n",
      "  [Epoch 03] step 30/66 loss=0.1690\n",
      "  [Epoch 03] step 40/66 loss=0.2113\n",
      "  [Epoch 03] step 50/66 loss=0.3117\n",
      "  [Epoch 03] step 60/66 loss=0.3699\n",
      "[Epoch 03] train_loss=0.2982  val_f1_DR=0.8401  val_acc_DR=0.8636  val_f1_DME=0.9471  val_acc_DME=0.9848\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 04] step 10/66 loss=0.1130\n",
      "  [Epoch 04] step 20/66 loss=0.0553\n",
      "  [Epoch 04] step 30/66 loss=0.1776\n",
      "  [Epoch 04] step 40/66 loss=0.0686\n",
      "  [Epoch 04] step 50/66 loss=0.1415\n",
      "  [Epoch 04] step 60/66 loss=0.3595\n",
      "[Epoch 04] train_loss=0.1681  val_f1_DR=0.8688  val_acc_DR=0.8990  val_f1_DME=0.9228  val_acc_DME=0.9747\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 05] step 10/66 loss=0.0621\n",
      "  [Epoch 05] step 20/66 loss=0.2695\n",
      "  [Epoch 05] step 30/66 loss=0.0215\n",
      "  [Epoch 05] step 40/66 loss=0.0821\n",
      "  [Epoch 05] step 50/66 loss=0.0525\n",
      "  [Epoch 05] step 60/66 loss=0.0245\n",
      "[Epoch 05] train_loss=0.0900  val_f1_DR=0.9016  val_acc_DR=0.9141  val_f1_DME=0.9560  val_acc_DME=0.9798\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 06] step 10/66 loss=0.1473\n",
      "  [Epoch 06] step 20/66 loss=0.1000\n",
      "  [Epoch 06] step 30/66 loss=0.0493\n",
      "  [Epoch 06] step 40/66 loss=0.0234\n",
      "  [Epoch 06] step 50/66 loss=0.0932\n",
      "  [Epoch 06] step 60/66 loss=0.0080\n",
      "[Epoch 06] train_loss=0.1151  val_f1_DR=0.9087  val_acc_DR=0.9141  val_f1_DME=0.9001  val_acc_DME=0.9697\n",
      "  [Epoch 07] step 10/66 loss=0.0112\n",
      "  [Epoch 07] step 20/66 loss=0.1788\n",
      "  [Epoch 07] step 30/66 loss=0.0826\n",
      "  [Epoch 07] step 40/66 loss=0.0588\n",
      "  [Epoch 07] step 50/66 loss=0.0379\n",
      "  [Epoch 07] step 60/66 loss=0.0069\n",
      "[Epoch 07] train_loss=0.0787  val_f1_DR=0.9081  val_acc_DR=0.9242  val_f1_DME=0.9324  val_acc_DME=0.9798\n",
      "  [Epoch 08] step 10/66 loss=0.0273\n",
      "  [Epoch 08] step 20/66 loss=0.4486\n",
      "  [Epoch 08] step 30/66 loss=0.0255\n",
      "  [Epoch 08] step 40/66 loss=0.2103\n",
      "  [Epoch 08] step 50/66 loss=0.0255\n",
      "  [Epoch 08] step 60/66 loss=0.0298\n",
      "[Epoch 08] train_loss=0.0797  val_f1_DR=0.9296  val_acc_DR=0.9394  val_f1_DME=0.9119  val_acc_DME=0.9697\n",
      "  [Epoch 09] step 10/66 loss=0.0059\n",
      "  [Epoch 09] step 20/66 loss=0.0092\n",
      "  [Epoch 09] step 30/66 loss=0.0097\n",
      "  [Epoch 09] step 40/66 loss=0.0133\n",
      "  [Epoch 09] step 50/66 loss=0.0214\n",
      "  [Epoch 09] step 60/66 loss=0.1021\n",
      "[Epoch 09] train_loss=0.0588  val_f1_DR=0.9123  val_acc_DR=0.9192  val_f1_DME=0.9388  val_acc_DME=0.9798\n",
      "  [Epoch 10] step 10/66 loss=0.0113\n",
      "  [Epoch 10] step 20/66 loss=0.0279\n",
      "  [Epoch 10] step 30/66 loss=0.0317\n",
      "  [Epoch 10] step 40/66 loss=0.0139\n",
      "  [Epoch 10] step 50/66 loss=0.0025\n",
      "  [Epoch 10] step 60/66 loss=0.0396\n",
      "[Epoch 10] train_loss=0.0468  val_f1_DR=0.9206  val_acc_DR=0.9293  val_f1_DME=0.9061  val_acc_DME=0.9747\n",
      "  [Epoch 11] step 10/66 loss=0.0051\n",
      "  [Epoch 11] step 20/66 loss=0.0174\n",
      "  [Epoch 11] step 30/66 loss=0.0677\n",
      "  [Epoch 11] step 40/66 loss=0.0149\n",
      "  [Epoch 11] step 50/66 loss=0.0300\n",
      "  [Epoch 11] step 60/66 loss=0.0317\n",
      "[Epoch 11] train_loss=0.0325  val_f1_DR=0.9355  val_acc_DR=0.9444  val_f1_DME=0.9626  val_acc_DME=0.9848\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 12] step 10/66 loss=0.4427\n",
      "  [Epoch 12] step 20/66 loss=0.0014\n",
      "  [Epoch 12] step 30/66 loss=0.0224\n",
      "  [Epoch 12] step 40/66 loss=0.0090\n",
      "  [Epoch 12] step 50/66 loss=0.0154\n",
      "  [Epoch 12] step 60/66 loss=0.0105\n",
      "[Epoch 12] train_loss=0.0794  val_f1_DR=0.9127  val_acc_DR=0.9192  val_f1_DME=0.9435  val_acc_DME=0.9798\n",
      "  [Epoch 13] step 10/66 loss=0.2372\n",
      "  [Epoch 13] step 20/66 loss=0.0081\n",
      "  [Epoch 13] step 30/66 loss=0.0443\n",
      "  [Epoch 13] step 40/66 loss=0.0044\n",
      "  [Epoch 13] step 50/66 loss=0.1356\n",
      "  [Epoch 13] step 60/66 loss=0.0210\n",
      "[Epoch 13] train_loss=0.0645  val_f1_DR=0.8880  val_acc_DR=0.8990  val_f1_DME=0.9157  val_acc_DME=0.9747\n",
      "  [Epoch 14] step 10/66 loss=0.0212\n",
      "  [Epoch 14] step 20/66 loss=0.0109\n",
      "  [Epoch 14] step 30/66 loss=0.0168\n",
      "  [Epoch 14] step 40/66 loss=0.0194\n",
      "  [Epoch 14] step 50/66 loss=0.0021\n",
      "  [Epoch 14] step 60/66 loss=0.0507\n",
      "[Epoch 14] train_loss=0.0265  val_f1_DR=0.9081  val_acc_DR=0.9242  val_f1_DME=0.9387  val_acc_DME=0.9798\n",
      "  [Epoch 15] step 10/66 loss=0.0050\n",
      "  [Epoch 15] step 20/66 loss=0.0046\n",
      "  [Epoch 15] step 30/66 loss=0.0100\n",
      "  [Epoch 15] step 40/66 loss=0.0027\n",
      "  [Epoch 15] step 50/66 loss=0.0009\n",
      "  [Epoch 15] step 60/66 loss=0.0044\n",
      "[Epoch 15] train_loss=0.0346  val_f1_DR=0.9260  val_acc_DR=0.9343  val_f1_DME=0.9265  val_acc_DME=0.9798\n",
      "  [Epoch 16] step 10/66 loss=0.0160\n",
      "  [Epoch 16] step 20/66 loss=0.0026\n",
      "  [Epoch 16] step 30/66 loss=0.0101\n",
      "  [Epoch 16] step 40/66 loss=0.0010\n",
      "  [Epoch 16] step 50/66 loss=0.0020\n",
      "  [Epoch 16] step 60/66 loss=0.0083\n",
      "[Epoch 16] train_loss=0.0299  val_f1_DR=0.9037  val_acc_DR=0.9192  val_f1_DME=0.9435  val_acc_DME=0.9798\n",
      "  [Epoch 17] step 10/66 loss=0.0070\n",
      "  [Epoch 17] step 20/66 loss=0.0190\n",
      "  [Epoch 17] step 30/66 loss=0.0257\n",
      "  [Epoch 17] step 40/66 loss=0.0019\n",
      "  [Epoch 17] step 50/66 loss=0.3536\n",
      "  [Epoch 17] step 60/66 loss=0.0038\n",
      "[Epoch 17] train_loss=0.0209  val_f1_DR=0.9179  val_acc_DR=0.9293  val_f1_DME=0.9788  val_acc_DME=0.9899\n",
      "  [Epoch 18] step 10/66 loss=0.0122\n",
      "  [Epoch 18] step 20/66 loss=0.0182\n",
      "  [Epoch 18] step 30/66 loss=0.1938\n",
      "  [Epoch 18] step 40/66 loss=0.0034\n",
      "  [Epoch 18] step 50/66 loss=0.0078\n",
      "  [Epoch 18] step 60/66 loss=0.0039\n",
      "[Epoch 18] train_loss=0.0204  val_f1_DR=0.9299  val_acc_DR=0.9394  val_f1_DME=0.9964  val_acc_DME=0.9949\n",
      "  ↳ Saved checkpoint\n",
      "  [Epoch 19] step 10/66 loss=0.0028\n",
      "  [Epoch 19] step 20/66 loss=0.0122\n",
      "  [Epoch 19] step 30/66 loss=0.0025\n",
      "  [Epoch 19] step 40/66 loss=0.0100\n",
      "  [Epoch 19] step 50/66 loss=0.0153\n",
      "  [Epoch 19] step 60/66 loss=0.0115\n",
      "[Epoch 19] train_loss=0.0180  val_f1_DR=0.8842  val_acc_DR=0.9091  val_f1_DME=0.9772  val_acc_DME=0.9899\n",
      "  [Epoch 20] step 10/66 loss=0.0067\n",
      "  [Epoch 20] step 20/66 loss=0.0405\n",
      "  [Epoch 20] step 30/66 loss=0.0064\n",
      "  [Epoch 20] step 40/66 loss=0.0016\n",
      "  [Epoch 20] step 50/66 loss=0.0080\n",
      "  [Epoch 20] step 60/66 loss=0.2667\n",
      "[Epoch 20] train_loss=0.0217  val_f1_DR=0.9122  val_acc_DR=0.9242  val_f1_DME=0.9929  val_acc_DME=0.9899\n",
      "  [Epoch 21] step 10/66 loss=0.0013\n",
      "  [Epoch 21] step 20/66 loss=0.0026\n",
      "  [Epoch 21] step 30/66 loss=0.0408\n",
      "  [Epoch 21] step 40/66 loss=0.0088\n",
      "  [Epoch 21] step 50/66 loss=0.0112\n",
      "  [Epoch 21] step 60/66 loss=0.0021\n",
      "[Epoch 21] train_loss=0.0272  val_f1_DR=0.9122  val_acc_DR=0.9242  val_f1_DME=0.9964  val_acc_DME=0.9949\n",
      "  [Epoch 22] step 10/66 loss=0.0007\n",
      "  [Epoch 22] step 20/66 loss=0.0467\n",
      "  [Epoch 22] step 30/66 loss=0.0102\n",
      "  [Epoch 22] step 40/66 loss=0.0014\n",
      "  [Epoch 22] step 50/66 loss=0.0008\n",
      "  [Epoch 22] step 60/66 loss=0.0046\n",
      "[Epoch 22] train_loss=0.0165  val_f1_DR=0.9208  val_acc_DR=0.9293  val_f1_DME=0.9737  val_acc_DME=0.9848\n",
      "  [Epoch 23] step 10/66 loss=0.0033\n",
      "  [Epoch 23] step 20/66 loss=0.0052\n",
      "  [Epoch 23] step 30/66 loss=0.0015\n",
      "  [Epoch 23] step 40/66 loss=0.0004\n",
      "  [Epoch 23] step 50/66 loss=0.0046\n",
      "  [Epoch 23] step 60/66 loss=0.0023\n",
      "[Epoch 23] train_loss=0.0081  val_f1_DR=0.9083  val_acc_DR=0.9192  val_f1_DME=0.9737  val_acc_DME=0.9848\n",
      "  [Epoch 24] step 10/66 loss=0.0220\n",
      "  [Epoch 24] step 20/66 loss=0.0048\n",
      "  [Epoch 24] step 30/66 loss=0.0081\n",
      "  [Epoch 24] step 40/66 loss=0.0012\n",
      "  [Epoch 24] step 50/66 loss=0.0007\n",
      "  [Epoch 24] step 60/66 loss=0.0188\n",
      "[Epoch 24] train_loss=0.0102  val_f1_DR=0.9208  val_acc_DR=0.9293  val_f1_DME=0.9737  val_acc_DME=0.9848\n",
      "  [Epoch 25] step 10/66 loss=0.0026\n",
      "  [Epoch 25] step 20/66 loss=0.0833\n",
      "  [Epoch 25] step 30/66 loss=0.0015\n",
      "  [Epoch 25] step 40/66 loss=0.0022\n",
      "  [Epoch 25] step 50/66 loss=0.0038\n",
      "  [Epoch 25] step 60/66 loss=0.0008\n",
      "[Epoch 25] train_loss=0.0124  val_f1_DR=0.9122  val_acc_DR=0.9242  val_f1_DME=0.9964  val_acc_DME=0.9949\n",
      "[EarlyStop] No improvement for 7 epochs.\n"
     ]
    }
   ],
   "source": [
    "scheduler = None   # start with no scheduler\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    # -------------------------------\n",
    "    # WARMUP → UNFREEZE\n",
    "    # -------------------------------\n",
    "    if epoch == WARMUP_EPOCHS + 1:\n",
    "        print(f\"[Info] Warmup finished. Unfreezing backbone for finetuning (LR={LR_FINETUNE}).\")\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LR_FINETUNE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=EPOCHS - WARMUP_EPOCHS,\n",
    "            eta_min=LR_FINETUNE * 0.1\n",
    "        )\n",
    "\n",
    "    # -------------------------------\n",
    "    # TRAIN\n",
    "    # -------------------------------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for step, (imgs, labels_dr, labels_dme) in enumerate(train_loader):\n",
    "        imgs       = imgs.to(device, non_blocking=True)\n",
    "        labels_dr  = labels_dr.to(device, non_blocking=True)\n",
    "        labels_dme = labels_dme.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            logits_dr, logits_dme = model(imgs)\n",
    "            loss_dr  = criterion_dr(logits_dr, labels_dr)\n",
    "            loss_dme = criterion_dme(logits_dme, labels_dme)\n",
    "            loss = loss_dr + LOSS_WEIGHT_DME * loss_dme\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"  [Epoch {epoch:02d}] step {step+1}/{len(train_loader)} loss={loss.item():.4f}\")\n",
    "\n",
    "    train_loss = total_loss / max(total_samples, 1)\n",
    "\n",
    "    # -------------------------------\n",
    "    # VALIDATION\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    all_dr_labels  = []\n",
    "    all_dr_preds   = []\n",
    "    all_dme_labels = []\n",
    "    all_dme_preds  = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels_dr, labels_dme in val_loader:\n",
    "            imgs       = imgs.to(device, non_blocking=True)\n",
    "            labels_dr  = labels_dr.to(device, non_blocking=True)\n",
    "            labels_dme = labels_dme.to(device, non_blocking=True)\n",
    "\n",
    "            logits_dr, logits_dme = model(imgs)\n",
    "            preds_dr  = logits_dr.argmax(dim=1)\n",
    "            preds_dme = logits_dme.argmax(dim=1)\n",
    "\n",
    "            all_dr_labels.append(labels_dr.cpu())\n",
    "            all_dr_preds.append(preds_dr.cpu())\n",
    "            all_dme_labels.append(labels_dme.cpu())\n",
    "            all_dme_preds.append(preds_dme.cpu())\n",
    "\n",
    "    all_dr_labels  = torch.cat(all_dr_labels).numpy()\n",
    "    all_dr_preds   = torch.cat(all_dr_preds).numpy()\n",
    "    all_dme_labels = torch.cat(all_dme_labels).numpy()\n",
    "    all_dme_preds  = torch.cat(all_dme_preds).numpy()\n",
    "\n",
    "    val_acc_dr = accuracy_score(all_dr_labels, all_dr_preds)\n",
    "    val_f1_dr  = f1_score(all_dr_labels, all_dr_preds, average=\"macro\")\n",
    "\n",
    "    val_acc_dme = accuracy_score(all_dme_labels, all_dme_preds)\n",
    "    val_f1_dme  = f1_score(all_dme_labels, all_dme_preds, average=\"macro\")\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch:02d}] train_loss={train_loss:.4f}  \"\n",
    "        f\"val_f1_DR={val_f1_dr:.4f}  val_acc_DR={val_acc_dr:.4f}  \"\n",
    "        f\"val_f1_DME={val_f1_dme:.4f}  val_acc_DME={val_acc_dme:.4f}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # SCHEDULER STEP\n",
    "    # -------------------------------\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    # -------------------------------\n",
    "    # EARLY STOP / CHECKPOINT\n",
    "    # -------------------------------\n",
    "    avg_f1 = 0.5 * (val_f1_dr + val_f1_dme)\n",
    "    if avg_f1 > best_f1 + 1e-5:\n",
    "        best_f1 = avg_f1\n",
    "        waited = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_f1_dr\":  float(val_f1_dr),\n",
    "                \"val_f1_dme\": float(val_f1_dme),\n",
    "            },\n",
    "            CKPT_PATH,\n",
    "        )\n",
    "        print(\"  ↳ Saved checkpoint\")\n",
    "    else:\n",
    "        waited += 1\n",
    "        if waited >= PATIENCE:\n",
    "            print(f\"[EarlyStop] No improvement for {PATIENCE} epochs.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343698d1",
   "metadata": {},
   "source": [
    "11. Load best and full evaluation (val + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bda0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded best checkpoint from epoch 18 | val_f1_dr=0.9299  val_f1_dme=0.9964\n"
     ]
    }
   ],
   "source": [
    "if CKPT_PATH.exists():\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    print(f\"[Info] Loaded best checkpoint from epoch {ckpt['epoch']} | \"\n",
    "          f\"val_f1_dr={ckpt.get('val_f1_dr', float('nan')):.4f}  \"\n",
    "          f\"val_f1_dme={ckpt.get('val_f1_dme', float('nan')):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e265a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Validation results (DR) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9815    0.9907        54\n",
      "           1     1.0000    1.0000    1.0000        40\n",
      "           2     0.9592    0.8704    0.9126        54\n",
      "           3     0.7500    1.0000    0.8571        30\n",
      "           4     1.0000    0.8000    0.8889        20\n",
      "\n",
      "    accuracy                         0.9394       198\n",
      "   macro avg     0.9418    0.9304    0.9299       198\n",
      "weighted avg     0.9510    0.9394    0.9408       198\n",
      "\n",
      "\n",
      "===== Validation results (DME) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9898    0.9949        98\n",
      "           1     1.0000    1.0000    1.0000        10\n",
      "           2     0.9890    1.0000    0.9945        90\n",
      "\n",
      "    accuracy                         0.9949       198\n",
      "   macro avg     0.9963    0.9966    0.9964       198\n",
      "weighted avg     0.9950    0.9949    0.9950       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation report (DR + DME)\n",
    "model.eval()\n",
    "all_dr_labels  = []\n",
    "all_dr_preds   = []\n",
    "all_dme_labels = []\n",
    "all_dme_preds  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels_dr, labels_dme in val_loader:\n",
    "        imgs       = imgs.to(device, non_blocking=True)\n",
    "        labels_dr  = labels_dr.to(device, non_blocking=True)\n",
    "        labels_dme = labels_dme.to(device, non_blocking=True)\n",
    "\n",
    "        logits_dr, logits_dme = model(imgs)\n",
    "        preds_dr  = logits_dr.argmax(dim=1)\n",
    "        preds_dme = logits_dme.argmax(dim=1)\n",
    "\n",
    "        all_dr_labels.append(labels_dr.cpu())\n",
    "        all_dr_preds.append(preds_dr.cpu())\n",
    "        all_dme_labels.append(labels_dme.cpu())\n",
    "        all_dme_preds.append(preds_dme.cpu())\n",
    "\n",
    "all_dr_labels  = torch.cat(all_dr_labels).numpy()\n",
    "all_dr_preds   = torch.cat(all_dr_preds).numpy()\n",
    "all_dme_labels = torch.cat(all_dme_labels).numpy()\n",
    "all_dme_preds  = torch.cat(all_dme_preds).numpy()\n",
    "\n",
    "val_report_dr  = classification_report(all_dr_labels,  all_dr_preds,  digits=4)\n",
    "val_cm_dr      = confusion_matrix(all_dr_labels,       all_dr_preds)\n",
    "val_acc_dr     = accuracy_score(all_dr_labels,         all_dr_preds)\n",
    "val_f1_dr      = f1_score(all_dr_labels,               all_dr_preds,  average=\"macro\")\n",
    "\n",
    "val_report_dme = classification_report(all_dme_labels, all_dme_preds, digits=4)\n",
    "val_cm_dme     = confusion_matrix(all_dme_labels,      all_dme_preds)\n",
    "val_acc_dme    = accuracy_score(all_dme_labels,        all_dme_preds)\n",
    "val_f1_dme     = f1_score(all_dme_labels,              all_dme_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n===== Validation results (DR) =====\")\n",
    "print(val_report_dr)\n",
    "\n",
    "print(\"\\n===== Validation results (DME) =====\")\n",
    "print(val_report_dme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b319ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Test results (DR) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6304    0.8529    0.7250        34\n",
      "           1     0.0000    0.0000    0.0000         5\n",
      "           2     0.5405    0.6250    0.5797        32\n",
      "           3     0.5000    0.3158    0.3871        19\n",
      "           4     0.4286    0.2308    0.3000        13\n",
      "\n",
      "    accuracy                         0.5631       103\n",
      "   macro avg     0.4199    0.4049    0.3984       103\n",
      "weighted avg     0.5224    0.5631    0.5287       103\n",
      "\n",
      "===== Test results (DME) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8039    0.9111    0.8542        45\n",
      "           1     0.3750    0.3000    0.3333        10\n",
      "           2     0.8864    0.8125    0.8478        48\n",
      "\n",
      "    accuracy                         0.8058       103\n",
      "   macro avg     0.6884    0.6745    0.6784       103\n",
      "weighted avg     0.8007    0.8058    0.8006       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test report (DR + DME)\n",
    "all_dr_labels  = []\n",
    "all_dr_preds   = []\n",
    "all_dme_labels = []\n",
    "all_dme_preds  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels_dr, labels_dme in test_loader:\n",
    "        imgs       = imgs.to(device, non_blocking=True)\n",
    "        labels_dr  = labels_dr.to(device, non_blocking=True)\n",
    "        labels_dme = labels_dme.to(device, non_blocking=True)\n",
    "\n",
    "        logits_dr, logits_dme = model(imgs)\n",
    "        preds_dr  = logits_dr.argmax(dim=1)\n",
    "        preds_dme = logits_dme.argmax(dim=1)\n",
    "\n",
    "        all_dr_labels.append(labels_dr.cpu())\n",
    "        all_dr_preds.append(preds_dr.cpu())\n",
    "        all_dme_labels.append(labels_dme.cpu())\n",
    "        all_dme_preds.append(preds_dme.cpu())\n",
    "\n",
    "all_dr_labels  = torch.cat(all_dr_labels).numpy()\n",
    "all_dr_preds   = torch.cat(all_dr_preds).numpy()\n",
    "all_dme_labels = torch.cat(all_dme_labels).numpy()\n",
    "all_dme_preds  = torch.cat(all_dme_preds).numpy()\n",
    "\n",
    "test_report_dr  = classification_report(all_dr_labels,  all_dr_preds,  digits=4)\n",
    "test_cm_dr      = confusion_matrix(all_dr_labels,       all_dr_preds)\n",
    "test_acc_dr     = accuracy_score(all_dr_labels,         all_dr_preds)\n",
    "test_f1_dr      = f1_score(all_dr_labels,               all_dr_preds,  average=\"macro\")\n",
    "\n",
    "test_report_dme = classification_report(all_dme_labels, all_dme_preds, digits=4)\n",
    "test_cm_dme     = confusion_matrix(all_dme_labels,      all_dme_preds)\n",
    "test_acc_dme    = accuracy_score(all_dme_labels,        all_dme_preds)\n",
    "test_f1_dme     = f1_score(all_dme_labels,              all_dme_preds, average=\"macro\")\n",
    "\n",
    "\n",
    "print(\"===== Test results (DR) =====\")\n",
    "print(test_report_dr)\n",
    "\n",
    "print(\"===== Test results (DME) =====\")\n",
    "print(test_report_dme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ce97deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Metrics saved to runs\\idrid_simple\\metrics_simple.json\n",
      "[Done] Training + evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "metrics_out = {\n",
    "    \"val\": {\n",
    "        \"dr\": {\n",
    "            \"acc\": float(val_acc_dr),\n",
    "            \"f1_macro\": float(val_f1_dr),\n",
    "            \"confusion_matrix\": val_cm_dr.tolist(),\n",
    "        },\n",
    "        \"dme\": {\n",
    "            \"acc\": float(val_acc_dme),\n",
    "            \"f1_macro\": float(val_f1_dme),\n",
    "            \"confusion_matrix\": val_cm_dme.tolist(),\n",
    "        }\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"dr\": {\n",
    "            \"acc\": float(test_acc_dr),\n",
    "            \"f1_macro\": float(test_f1_dr),\n",
    "            \"confusion_matrix\": test_cm_dr.tolist(),\n",
    "        },\n",
    "        \"dme\": {\n",
    "            \"acc\": float(test_acc_dme),\n",
    "            \"f1_macro\": float(test_f1_dme),\n",
    "            \"confusion_matrix\": test_cm_dme.tolist(),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RUN_DIR / \"metrics_simple.json\", \"w\") as f:\n",
    "    json.dump(metrics_out, f, indent=2)\n",
    "\n",
    "print(f\"[Info] Metrics saved to {RUN_DIR / 'metrics_simple.json'}\")\n",
    "print(\"[Done] Training + evaluation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
